{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import time\n",
    "from datetime import date, timedelta\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from fake_useragent import UserAgent\n",
    "import cloudscraper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',200)\n",
    "pd.set_option('display.max_colwidth',300)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nejm = 'https://www.nejm.org/coronavirus'\n",
    "corona = requests.get(nejm)\n",
    "pag_princ = bs(corona.content, \"html.parser\")\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "des = []\n",
    "autor = []\n",
    "data = []\n",
    "for contain in pag_princ.find_all(\"ul\", {\"o-curated-area\"}):\n",
    "    try:\n",
    "        title_row = contain.b.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "        link_row = contain.find('a', {'class': 'm-article__link'})['href']\n",
    "        des_row = contain.p.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "        autor_row = contain.find('em', {'class': 'm-article__author f-author'}).get_text()\n",
    "        data_row = contain.find('em', {'class': 'm-article__date f-tag'}).get_text()\n",
    "        title.append(title_row)\n",
    "        link.append(link_row)\n",
    "        des.append(des_row)\n",
    "        autor.append(autor_row)\n",
    "        data.append(data_row)\n",
    "        #print(\"title: {}\\nlink: {}\\ndescription: {}\\n\".format(title,link,des))\n",
    "    except Exception as e:\n",
    "        print('Error, skipping...', e)\n",
    "for contain in pag_princ.find_all(\"li\", {\"m-result m-result--img separatorLines\"}):\n",
    "    try:\n",
    "        title_row = contain.strong.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "        link_row = contain.find('a', {'class': 'm-result__link'})['href']\n",
    "        des_row = contain.find('span', {'class': 'm-result__blurb f-blurb'}).find('p').get_text()\n",
    "        autor_row = contain.find('em', {'class': 'm-result__author f-author'}).get_text()\n",
    "        data_row = contain.find('em', {'class': 'm-result__date f-tag'}).get_text()\n",
    "        title.append(title_row)\n",
    "        link.append(link_row)\n",
    "        des.append(des_row)\n",
    "        autor.append(autor_row)\n",
    "        data.append(data_row)\n",
    "        #print(\"title: {}\\nlink: {}\\ndescription: {}\\n\".format(title,link,des))\n",
    "    except Exception as e:\n",
    "        print('Error, skipping...', e)        \n",
    "         \n",
    "df_corona_news = pd.DataFrame({'Data':data,'Titulo':title,'Descricao':des,'Autor':autor,'Link':link })\n",
    "df_corona_news = df_corona_news.astype(str)\n",
    "df_corona_news['Descricao'] = df_corona_news['Descricao'].str.replace('\\n ', '')\n",
    "df_corona_news['Titulo'] = df_corona_news['Titulo'].str.replace('\\n ', '')\n",
    "df_corona_news['Descricao'] = df_corona_news['Descricao'].str.strip()\n",
    "df_corona_news['Titulo'] = df_corona_news['Titulo'].str.strip()\n",
    "df_corona_news['Doi'] = df_corona_news['Link'].str.replace('/doi/full/', '')\n",
    "df_corona_news['Doi'] = df_corona_news['Doi'].str.replace('?', '')\n",
    "df_corona_news['Doi'] = df_corona_news['Doi'].str.replace('query=featured_coronavirus', '')\n",
    "df_corona_news['Doi'] = df_corona_news['Doi'].str.replace('query=featured_home', '')\n",
    "df_corona_news = df_corona_news[['Data','Titulo','Descricao','Autor','Doi','Link']]\n",
    "df_corona_news['Link'] = 'https://www.nejm.org' + df_corona_news['Link'].astype(str)\n",
    "df_corona_news['Fonte'] = 'NEJM'\n",
    "news_1 = df_corona_news\n",
    "news_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('The_New_England_Journal_of_Medicine.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ua = UserAgent()\n",
    "header = {\n",
    "    \"User-Agent\": ua.random\n",
    "}\n",
    "r = requests.get('https://www.jamanetwork.com/journals/jama/pages/coronavirus-alert', headers=header)\n",
    "\n",
    "pag_princ = bs(r.content, \"html.parser\")\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "# des = []\n",
    "autor = []\n",
    "# data = []\n",
    "for contain in pag_princ.find_all(\"div\", {\"alert-info\"}):\n",
    "    try:\n",
    "        title_row = contain.a.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "        link_row = contain.a[\"href\"]\n",
    "#         des_row = contain.p.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "        autor_row = contain.find('span')\n",
    "#         data_row = contain.find('em', {'class': 'm-article__date f-tag'}).get_text()\n",
    "        title.append(title_row)\n",
    "        link.append(link_row)\n",
    "#         des.append(des_row)\n",
    "        autor.append(autor_row)\n",
    "#         data.append(data_row)\n",
    "        #print(\"title: {}\\nlink: {}\\ndescription: {}\\n\".format(title,link,des))\n",
    "    except Exception as e:\n",
    "        print('Error, skipping...', e)  \n",
    "         \n",
    "df_corona_news = pd.DataFrame({'Titulo':title,'Autor':autor,'Link':link })\n",
    "df_corona_news['Autor'] = df_corona_news['Autor'].astype(str)\n",
    "df_corona_news['Autor'] = df_corona_news['Autor'].str.replace('<span>', '')\n",
    "df_corona_news['Autor'] = df_corona_news['Autor'].str.replace('</span>', '')\n",
    "df_corona_news = df_corona_news[df_corona_news['Link'].str.contains('/journals')]\n",
    "df_corona_news['Link'] = 'https://jamanetwork.com' + df_corona_news['Link'].astype(str)\n",
    "df_corona_news['Fonte'] = 'JAMA'\n",
    "news_2 = df_corona_news\n",
    "news_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('Jama_Network.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "s = requests.Session()\n",
    "s.headers.update({'x-test': 'true'})\n",
    "\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "# des = []\n",
    "autor = []\n",
    "data = []\n",
    "\n",
    "i=-1\n",
    "while True:\n",
    "    i = i+1\n",
    "    url = \"https://www.thelancet.com/coronavirus/archive?startPage=\" + str(i)\n",
    "    corona = requests_retry_session(session=s).get(url)\n",
    "    if corona.status_code != 200:\n",
    "        break\n",
    "    corona = requests_retry_session(session=s).get(url)\n",
    "    pag_princ = bs(corona.content, \"html.parser\")\n",
    "#     print('Lendo página...', i)\n",
    "    if pag_princ.find_all(\"div\", {\"articleCitation\"}) == []:\n",
    "        break\n",
    "    for contain in pag_princ.find_all(\"div\", {\"articleCitation\"}):\n",
    "        try:\n",
    "            title_row = contain.a.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "            link_row = contain.a[\"href\"]\n",
    "    #         des_row = contain.p.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "            autor_row = contain.find('div', {'class': 'authors'}).get_text()\n",
    "            data_row = contain.find('div', {'class': 'published-online'}).get_text()\n",
    "            title.append(title_row)\n",
    "            link.append(link_row)\n",
    "    #         des.append(des_row)\n",
    "            autor.append(autor_row)\n",
    "            data.append(data_row)\n",
    "#             print(\"data: {}\\ntitle: {}\\nautor: {}\\nlink: {}\\n\".format(data,title,autor,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "\n",
    "df_corona_news = pd.DataFrame({'Data':data,'Titulo':title,'Autor':autor,'Link':link })\n",
    "df_corona_news['Autor'] = df_corona_news['Autor'].astype(str)\n",
    "df_corona_news['Autor'] = df_corona_news['Autor'].str.replace('<span>', '')\n",
    "df_corona_news['Autor'] = df_corona_news['Autor'].str.replace('</span>', '')\n",
    "df_corona_news['Data'] = df_corona_news['Data'].astype(str)\n",
    "df_corona_news['Data'] = df_corona_news['Data'].str.replace('Published:', '')\n",
    "df_corona_news['Link'] = 'https://www.thelancet.com' + df_corona_news['Link'].astype(str)\n",
    "df_corona_news['Fonte'] = 'LANCET'\n",
    "news_3 = df_corona_news\n",
    "news_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('The_Lancet.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "# des = []\n",
    "autor = []\n",
    "# data = []\n",
    "\n",
    "s = requests.Session()\n",
    "s.headers.update({'x-test': 'true'})\n",
    "url = \"https://www.cell.com/2019-nCOV\"\n",
    "corona = requests_retry_session(session=s).get(url)\n",
    "pag_princ = bs(corona.content, \"html.parser\")\n",
    "\n",
    "for contain in pag_princ.find_all(\"div\", {\"article-details\"}):\n",
    "        try:\n",
    "            title_row = contain.a.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "            link_row = contain.a[\"href\"]\n",
    "    #         des_row = contain.p.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "            autor_row = contain.find('div', {'class': 'authors'}).get_text()\n",
    "#             data_row = contain.find('div', {'class': 'published-online'}).get_text()\n",
    "            title.append(title_row)\n",
    "            link.append(link_row)\n",
    "    #         des.append(des_row)\n",
    "            autor.append(autor_row)\n",
    "#             data.append(data_row)\n",
    "#             print(\"title: {}\\nautor: {}\\nlink: {}\\n\".format(title,autor,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "\n",
    "df_corona_news = pd.DataFrame({'Titulo':title,'Autor':autor,'Link':link })\n",
    "df_corona_news['Autor'] = df_corona_news['Autor'].astype(str)\n",
    "df_corona_news['Autor'] = df_corona_news['Autor'].str.replace('<span>', '')\n",
    "df_corona_news['Autor'] = df_corona_news['Autor'].str.replace('</span>', '')\n",
    "df_corona_news.loc[~df_corona_news['Link'].str.contains('//els-jbs-prod-cdn|https'), 'Link'] = 'https://www.cell.com' + df_corona_news['Link'].astype(str)\n",
    "df_corona_news['Link'] = df_corona_news['Link'].str.replace(':443', '')\n",
    "df_corona_news.loc[~df_corona_news['Link'].str.contains('https'), 'Link'] = 'https:' + df_corona_news['Link'].astype(str)\n",
    "df_corona_news['Fonte'] = 'CELL'\n",
    "news_4 = df_corona_news\n",
    "news_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('Cell.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "# des = []\n",
    "# autor = []\n",
    "data = []\n",
    "\n",
    "s = requests.Session()\n",
    "s.headers.update({'x-test': 'true'})\n",
    "url = \"https://www.bmj.com/archive/sevendays\"\n",
    "corona = requests_retry_session(session=s).get(url)\n",
    "pag_princ = bs(corona.content, \"html.parser\")\n",
    "\n",
    "for contain in pag_princ.find_all(\"div\", {\"highwire-cite highwire-cite-highwire-article highwire-citation-bmj-toc-list clearfix\"}):\n",
    "        try:\n",
    "            title_row = contain.find('span', {'class': 'highwire-cite-title'}).get_text()\n",
    "            link_row = contain.find('a', {'class': 'highwire-cite-linked-title'})['href']\n",
    "    #         des_row = contain.p.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "#             autor_row = contain.find('div', {'class': 'authors'}).get_text()\n",
    "            data_row = contain.find('span', {'class': 'highwire-cite-metadata-date highwire-cite-metadata'}).find_next_sibling(\"span\").find_next_sibling(\"span\").find_next_sibling(\"span\").get_text()\n",
    "            title.append(title_row)\n",
    "            link.append(link_row)\n",
    "    #         des.append(des_row)\n",
    "#             autor.append(autor_row)\n",
    "            data.append(data_row)\n",
    "#             print(\"data: {}\\ntitle: {}\\nlink: {}\\n\".format(data,title,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "\n",
    "df_corona_news = pd.DataFrame({'Data':data,'Titulo':title,'Link':link })\n",
    "df_corona_news['Data'] = df_corona_news['Data'].astype(str)\n",
    "df_corona_news['Data'] = df_corona_news['Data'].str.replace('(', '')\n",
    "df_corona_news['Data'] = df_corona_news['Data'].str.replace(')', '')\n",
    "df_corona_news['Data'] = df_corona_news['Data'].str.replace('Published', '')\n",
    "df_corona_news['Link'] = 'https://www.bmj.com' + df_corona_news['Link'].astype(str)\n",
    "df_corona_news = df_corona_news[df_corona_news['Titulo'].str.contains('covid-19|Covid-19|coronavirus|Coronavirus')]\n",
    "df_corona_news['Fonte'] = 'BMJ'\n",
    "news_5 = df_corona_news\n",
    "news_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('bmj.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=10,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "ua = UserAgent()\n",
    "s = requests.Session()\n",
    "s.headers.update({\"User-Agent\": ua.random})\n",
    "\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "des = []\n",
    "autor = []\n",
    "data = []\n",
    "\n",
    "i=0\n",
    "while True:\n",
    "    i = i+1\n",
    "    url = \"https://academic.oup.com/journals/search-results?q=coronavirus&sort=Date+–+Newest+First&allJournals=1&fl_SiteID=5567&page=\" + str(i)\n",
    "    corona = requests_retry_session(session=s).get(url)\n",
    "    if corona.status_code != 200:\n",
    "        break\n",
    "    corona = requests_retry_session(session=s).get(url)\n",
    "    pag_princ = bs(corona.content, \"html.parser\")\n",
    "    print('Lendo página...', i)\n",
    "    if pag_princ.find_all(\"div\", {\"sr-list al-article-box al-normal clearfix\"}) == []:\n",
    "        break\n",
    "    for contain in pag_princ.find_all(\"div\", {\"sr-list al-article-box al-normal clearfix\"}):\n",
    "        try:\n",
    "            title_row = contain.find('a', {'class': 'article-link'}).get_text()\n",
    "            link_row = contain.a[\"href\"]\n",
    "            des_row = contain.find('div', {'class': 'snippet'}).get_text()\n",
    "            autor_row = contain.find('div', {'class': 'sri-authors al-authors-list'}).get_text()\n",
    "            data_row = contain.find('div', {'class': 'sri-date al-pub-date'}).get_text()\n",
    "            title.append(title_row)\n",
    "            link.append(link_row)\n",
    "            des.append(des_row)\n",
    "            autor.append(autor_row)\n",
    "            data.append(data_row)\n",
    "#             print(\"data: {}\\ntitle: {}\\ndes: {}\\nautor: {}\\nlink: {}\\n\".format(data,title,des,autor,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "\n",
    "df_corona_news = pd.DataFrame({'Data':data,'Titulo':title,'Descricao':des,'Autor':autor,'Link':link })\n",
    "df_corona_news['Descricao'] = df_corona_news['Descricao'].astype(str)\n",
    "df_corona_news['Descricao'] = df_corona_news['Descricao'].str.replace('\\r', '')\n",
    "df_corona_news['Descricao'] = df_corona_news['Descricao'].str.replace('\\n', '')\n",
    "df_corona_news['Data'] = df_corona_news['Data'].astype(str)\n",
    "df_corona_news['Data'] = df_corona_news['Data'].str.replace('Published:', '')\n",
    "df_corona_news['Link'] = 'https://academic.oup.com' + df_corona_news['Link'].astype(str)\n",
    "df_corona_news['Fonte'] = 'OXFORD'\n",
    "news_6 = df_corona_news\n",
    "news_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('oxford.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=10,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "# des = []\n",
    "# autor = []\n",
    "#data = []\n",
    "\n",
    "s = cloudscraper.CloudScraper()\n",
    "ua = UserAgent()\n",
    "s.headers.update({\"User-Agent\": ua.random})\n",
    "url = \"https://www.cdc.gov/coronavirus/2019-ncov/communication/publications.html?CDC_AA_refVal=https%3A%2F%2Fwww.cdc.gov%2Fcoronavirus%2F2019-ncov%2Fpublications.html\"\n",
    "corona = requests_retry_session(session=s).get(url)\n",
    "pag_princ = bs(corona.content, \"html.parser\")\n",
    "\n",
    "ul = pag_princ.find(\"p\", text=\"COVID-19 MMWRs\").find_next_sibling(\"ul\")\n",
    "for contain in ul.find_all('li'):\n",
    "        try:\n",
    "            title_row = contain.get_text()\n",
    "            link_row = contain.find('a', {'class': 'tp-link-policy'})['href']\n",
    "    #         des_row = contain.p.text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "#             autor_row = contain.find('div', {'class': 'authors'}).get_text()\n",
    "#             data_row = contain.find('span', {'class': 'highwire-cite-metadata-date highwire-cite-metadata'}).find_next_sibling(\"span\").find_next_sibling(\"span\").find_next_sibling(\"span\").get_text()\n",
    "            title.append(title_row)\n",
    "            link.append(link_row)\n",
    "    #         des.append(des_row)\n",
    "#             autor.append(autor_row)\n",
    "#             data.append(data_row)\n",
    "#             print(\"title: {}\\nlink: {}\\n\".format(title,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "\n",
    "df_corona_news = pd.DataFrame({'Titulo_full':title,'Link':link})\n",
    "df_corona_news = df_corona_news.join(df_corona_news['Titulo_full'].str.split('.', expand=True).rename(columns={0:'Autor', 1:'Titulo'}))\n",
    "df_corona_news = df_corona_news[['Titulo','Autor','Link']]\n",
    "df_corona_news['Fonte'] = 'CDC'\n",
    "news_7 = df_corona_news\n",
    "news_7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('CDC.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "des = []\n",
    "autor = []\n",
    "data = []\n",
    "\n",
    "s = requests.Session()\n",
    "s.headers.update({'x-test': 'true'})\n",
    "url = \"https://www.nature.com/collections/hajgidghjb\"\n",
    "corona = requests_retry_session(session=s).get(url)\n",
    "pag_princ = bs(corona.content, \"html.parser\")\n",
    "\n",
    "for contain in pag_princ.find_all(\"article\", {\"cleared c-article-item flex-box flex-rr-nowrap mq640-kill-flex\"}):\n",
    "        try:\n",
    "            title_row = contain.a.get_text(strip=True)\n",
    "            link_row = contain.a[\"href\"]\n",
    "            des_row = contain.find('div', {'class': 'c-article-item__description tighten-line-height'}).get_text(strip=True)\n",
    "            autor_row = contain.find('ul', {'class': 'c-article-item__authors'}).get_text(strip=True)\n",
    "            data_row = contain.find('time', {'class': 'c-article-item__publication-date'}).get_text(strip=True)\n",
    "            title.append(title_row)\n",
    "            link.append(link_row)\n",
    "            des.append(des_row)\n",
    "            autor.append(autor_row)\n",
    "            data.append(data_row)\n",
    "#             print(\"data: {}\\ntitle: {}\\ndes: {}\\nautor: {}\\nlink: {}\\n\".format(data,title,des,autor,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "\n",
    "df_corona_news = pd.DataFrame({'Data':data,'Titulo':title,'Descricao':des,'Autor':autor,'Link':link })\n",
    "df_corona_news['Link'] = 'https://www.nature.com' + df_corona_news['Link'].astype(str)\n",
    "df_corona_news['Fonte'] = 'NATURE'\n",
    "news_8 = df_corona_news\n",
    "news_8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('Nature.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=10,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "doi = []\n",
    "autor = []\n",
    "# data = []\n",
    "\n",
    "s = cloudscraper.CloudScraper()\n",
    "s.headers.update({'x-test': 'true'})\n",
    "url = \"https://novel-coronavirus.onlinelibrary.wiley.com/\"\n",
    "corona = requests_retry_session(session=s).get(url)\n",
    "pag_princ = bs(corona.content, \"html.parser\")\n",
    "\n",
    "for contain in pag_princ.find_all(\"div\", {\"issue-item\"}):\n",
    "        try:\n",
    "            title_row = contain.find('a', {'class': 'issue-item__title visitable'}).get_text(strip=True)\n",
    "            link_row = contain.a[\"href\"]\n",
    "            doi_row = contain.find('li', {'class': 'coverDoi'}).get_text(strip=True)\n",
    "            autor_row = contain.find('ul', {'class': 'rlist--inline loa comma'}).get_text(strip=True)\n",
    "# #             data_row = contain.find('time', {'class': 'c-article-item__publication-date'}).get_text(strip=True)\n",
    "            title.append(title_row)\n",
    "            link.append(link_row)\n",
    "            doi.append(doi_row)\n",
    "            autor.append(autor_row)\n",
    "#             data.append(data_row)\n",
    "#             print(\"title: {}\\ndoi: {}\\nautor: {}\\nlink: {}\\n\".format(title,doi,autor,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "\n",
    "df_corona_news = pd.DataFrame({'Titulo':title,'Autor':autor,'Doi':doi,'Link':link })\n",
    "df_corona_news['Doi'] = df_corona_news['Doi'].astype(str)\n",
    "df_corona_news['Doi'] = df_corona_news['Doi'].str.replace('DOI:&nbsp', '')\n",
    "df_corona_news['Fonte'] = 'WILLEY'\n",
    "news_9 = df_corona_news\n",
    "news_9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('Novel_Coronavirus_Wiley_Online_Library.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=10,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "ua = UserAgent()\n",
    "s = requests.Session()\n",
    "s.headers.update({\"User-Agent\": ua.random})\n",
    "\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "# des = []\n",
    "autor = []\n",
    "data = []\n",
    "\n",
    "i=0\n",
    "while True:\n",
    "    i = i+1\n",
    "    url = \"https://www.cambridge.org/core/browse-subjects/medicine/coronavirus-free-access-collection?pageNum=\" + str(i)\n",
    "    corona = requests_retry_session(session=s).get(url)\n",
    "    if corona.status_code != 200:\n",
    "        break\n",
    "    corona = requests_retry_session(session=s).get(url)\n",
    "    pag_princ = bs(corona.content, \"html.parser\")\n",
    "    print('Lendo página...', i)\n",
    "    if pag_princ.find_all(\"ul\", {\"details\"}) == []:\n",
    "        break\n",
    "    for contain in pag_princ.find_all(\"ul\", {\"details\"}):\n",
    "        try:\n",
    "            title_row = contain.find('a', {'class': 'part-link'}).get_text(strip=True)\n",
    "            link_row = contain.a[\"href\"]\n",
    "#             des_row = contain.find('div', {'class': 'snippet'}).get_text()\n",
    "            autor_row = contain.find('a', {'class': 'more-by-this-author'}).get_text(strip=True)\n",
    "            data_row = contain.find('span', {'class': 'date'}).get_text(strip=True)\n",
    "            title.append(title_row)\n",
    "            link.append(link_row)\n",
    "#             des.append(des_row)\n",
    "            autor.append(autor_row)\n",
    "            data.append(data_row)\n",
    "#             print(\"data: {}\\ntitle: {}\\nautor: {}\\nlink: {}\\n\".format(data,title,autor,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "\n",
    "df_corona_news = pd.DataFrame({'Data':data,'Titulo':title,'Autor':autor,'Link':link })\n",
    "df_corona_news['Link'] = 'https://www.cambridge.org' + df_corona_news['Link'].astype(str)\n",
    "df_corona_news['Fonte'] = 'CAMBRIDGE'\n",
    "news_10 = df_corona_news\n",
    "news_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('Cambridge.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=10,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "ua = UserAgent()\n",
    "s = requests.Session()\n",
    "s.headers.update({\"User-Agent\": ua.random})\n",
    "\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "doi = []\n",
    "autor = []\n",
    "data = []\n",
    "\n",
    "i=0\n",
    "while True:\n",
    "    i = i+1\n",
    "    url = \"http://connect.medrxiv.org/relate/content/181?page=\" + str(i)\n",
    "    corona = requests_retry_session(session=s).get(url)\n",
    "    if corona.status_code != 200:\n",
    "        break\n",
    "    corona = requests_retry_session(session=s).get(url)\n",
    "    pag_princ = bs(corona.content, \"html.parser\")\n",
    "    print('Lendo página...', i)\n",
    "    if i==213:\n",
    "        break\n",
    "    for contain in pag_princ.find_all(\"div\", {\"highwire-article-citation highwire-citation-type-highwire-article\"}):\n",
    "        try:\n",
    "            title_row = contain.find('a', {'class': 'highwire-cite-linked-title'}).get_text(strip=True)\n",
    "            link_row = contain.a[\"href\"]\n",
    "#             des_row = contain.find('div', {'class': 'snippet'}).get_text()\n",
    "#             autor_row = contain.find('span', {'class': 'highwire-citation-authors'})\n",
    "#             data_row = contain.find('span', {'class': 'highwire-cite-metadata-journal'}).get_text(strip=True)\n",
    "            title.append(title_row)\n",
    "            link.append(link_row)\n",
    "#             des.append(des_row)\n",
    "#             autor.append(autor_row)\n",
    "#             data.append(data_row)\n",
    "#             print(\"data: {}\\ntitle: {}\\nautor: {}\\nlink: {}\\n\".format(data,title,autor,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "            \n",
    "    for contain in pag_princ.find_all(\"div\", {\"highwire-cite-authors\"}):\n",
    "        try:\n",
    "            autor_row = contain.find('span', {'class': 'highwire-citation-authors'}).get_text(strip=True)\n",
    "            autor.append(autor_row)\n",
    "#             print(\"data: {}\\ntitle: {}\\nautor: {}\\nlink: {}\\n\".format(data,title,autor,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "    for contain in pag_princ.find_all(\"div\", {\"highwire-cite-metadata\"}):\n",
    "        try:\n",
    "            doi_row = contain.find('span', {'class': 'highwire-cite-metadata-journal'}).find('a').get_text(strip=True)\n",
    "            data_row = contain.find('span', {'class': 'highwire-cite-metadata-journal'}).get_text(strip=True)\n",
    "            doi.append(doi_row)\n",
    "            data.append(data_row)\n",
    "#             print(\"data: {}\\ntitle: {}\\ndoi: {}nautor: {}\\nlink: {}\\n\".format(data,title,doi,autor,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "\n",
    "df_corona_news = pd.DataFrame({'Data':data,'Titulo':title,'Doi':doi,'Autor':autor,'Link':link })\n",
    "df_corona_news['Data'] = df_corona_news['Data'].str[-10:]\n",
    "df_corona_news['Fonte'] = 'medRxiv'\n",
    "news_11 = df_corona_news\n",
    "news_11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('medrxiv.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def requests_retry_session(\n",
    "    retries=10,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "\n",
    "title = []\n",
    "link = []\n",
    "# des = []\n",
    "# autor = []\n",
    "data = []\n",
    "\n",
    "i=-1\n",
    "while True:\n",
    "    i = i+1\n",
    "    ua = UserAgent()\n",
    "    s = requests.Session()\n",
    "    s.headers.update({'User-Agent':str(ua.random)})\n",
    "    url = \"https://www.ecdc.europa.eu/en/coronavirus?type%5B0%5D=1244&type%5B1%5D=1307&type%5B2%5D=1382&sort_by=field_ct_publication_date_value&items_per_page=4&pager_type=infinite_scroll&type_op=or&tid_op=or&tid%5B0%5D%5Btarget_id%5D=2943&sort_order=DESC&bid=MMgA0tBXpTBOCUYtiHvtZZMke0deSswQEdBwXsQNaQM&nid=33355&page=\" + str(i)\n",
    "    corona = requests_retry_session(session=s).get(url)\n",
    "    if corona.status_code != 200:\n",
    "        break\n",
    "    corona = requests_retry_session(session=s).get(url)\n",
    "    pag_princ = bs(corona.content, \"html.parser\")\n",
    "    pag_princ = pag_princ.find(\"div\", {\"col-xs-12 col-sm-10 col-sm-offset-1\"})\n",
    "    print('Lendo página...', i)\n",
    "    if pag_princ.find_all(\"article\", {\"ct--view-15 node node--type-ct-publication node--view-mode-vm-view15 ct\"}) == []:\n",
    "        break\n",
    "    for contain in pag_princ.find_all(\"article\", {\"ct--view-15 node node--type-ct-publication node--view-mode-vm-view15 ct\"}):\n",
    "        try:\n",
    "            title_row = contain.find('h3', {'class': 'ct__title'}).get_text(strip=True)\n",
    "            link_row = contain.find('a', {'class': 'ct__link'})[\"href\"]\n",
    "#             des_row = contain.find('div', {'class': 'c-article-item__description tighten-line-height'}).get_text(strip=True)\n",
    "#             autor_row = contain.find('ul', {'class': 'c-article-item__authors'}).get_text(strip=True)\n",
    "            data_row = contain.find('time', {'class': 'ct__meta__value'}).get_text(strip=True)\n",
    "            title.append(title_row)\n",
    "            link.append(link_row)\n",
    "#             des.append(des_row)\n",
    "#             autor.append(autor_row)\n",
    "            data.append(data_row)\n",
    "#             print(\"data: {}\\ntitle: {}\\nlink: {}\".format(data,title,link))\n",
    "        except Exception as e:\n",
    "            print('Error, skipping...', e)\n",
    "\n",
    "df_corona_news = pd.DataFrame({'Data':data,'Titulo':title,'Link':link })\n",
    "df_corona_news = df_corona_news.drop_duplicates()\n",
    "df_corona_news['Link'] = 'https://www.ecdc.europa.eu' + df_corona_news['Link'].astype(str)\n",
    "df_corona_news['Fonte'] = 'ECDC'\n",
    "news_12 = df_corona_news\n",
    "news_12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_corona_news.to_csv('ECDC.csv', sep = '|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_news = pd.concat([news_1,news_2,news_3,news_4,news_5,news_6,news_7,news_8,news_9,news_10,news_11,news_12]).reset_index(drop = True)\n",
    "full_news = full_news[['Data','Titulo','Descricao','Autor','Doi','Link','Fonte','Responsavel']]\n",
    "full_news = full_news.applymap(str)\n",
    "full_news = full_news.applymap(str.strip)\n",
    "full_news['count'] = pd.isnull(full_news).sum(1)\n",
    "full_news = full_news.sort_values(['count']).drop_duplicates(subset=['Titulo'],keep='first').drop('count',1)\n",
    "full_news = full_news.sort_values(\"Fonte\")\n",
    "full_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.Timestamp('today')\n",
    "full_news.to_csv('covida_news_{:%m%d%Y}'.format(today)+'.csv', sep = ';', encoding='utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = pd.Timestamp('today')\n",
    "full_news.to_excel(r'covida_news_{:%m%d%Y}'.format(today)+'.xlsx',sheet_name= 'covida_news_{:%m%d%Y}'.format(today))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2020-05-27'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yesterday = date.today() - timedelta(days=1)\n",
    "yesterday = yesterday.strftime('%Y-%m-%d')\n",
    "yesterday"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
